Hypothesis & Technical Uncertainty:
My core hypothesis going into this phase was that leveraging modern AI, specifically LLMs like GPT-4o-mini and specialized image generation APIs, could create a significantly more dynamic, engaging, and personalized language learning experience compared to traditional, static text adventures or flashcard apps. The goal wasn't just to build a game, but to explore the practicalities of integrating AI for generating core game content (locations, items, NPCs, descriptions) on the fly, providing contextual learning opportunities within an immersive environment.
Several technical uncertainties existed that couldn't be fully answered by readily available documentation or examples, representing potential valuable insights for the company:
Structured AI Output Reliability: Could an LLM consistently generate structured data (JSON) with specific, required fields (like separating English names from Romaji transliterations) based purely on prompt engineering? How much iteration and specific instruction is needed to achieve acceptable reliability for game logic consumption? Initial findings: This proved more challenging than anticipated, especially preventing the AI from conflating similar concepts like English names and Romaji. It required multiple, increasingly explicit prompt refinements with concrete examples, indicating that robust validation and potentially post-processing layers are essential for production use. Understanding these prompt nuances is key knowledge for developing future AI features efficiently.
Contextual Relevance vs. Curriculum Constraints: Could AI generate contextually relevant vocabulary suggestions (e.g., words related to a specific item or NPC) that also reliably map back to a predefined curriculum (like the JLPT N5 list)? Initial findings: The AI often suggested relevant but out-of-scope words, or words formatted differently than our vocabulary list expected. This highlights a core challenge in using generative AI for tightly constrained educational content. Our workaround (linking vocab via item names/NPC roles) was partially successful but less ideal than accurate AI suggestions, suggesting hybrid approaches or more sophisticated vocabulary mapping might be needed for learning applications.
Asynchronous AI Integration in Real-Time Web: How complex is the integration of multiple asynchronous AI calls (location details, item/NPC generation, image generation) within a real-time Flask/SocketIO web application? What are the potential race conditions or performance bottlenecks? Initial findings: We encountered issues like the starting location not being ready when the WebSocket connected, requiring explicit checks and waits. Background threading for image generation was necessary for UI responsiveness. This practical experience in managing concurrent AI tasks in a web context is valuable for architecting similar applications, highlighting the need for careful state management and potentially task queue systems.
What Worked Well:
Core AI Content Generation: The fundamental goal was achieved â€“ the game successfully uses AI to generate unique locations, items, and NPCs dynamically, providing varied descriptions and context. This validates the core hypothesis about AI's potential for dynamic content.
AI-Powered NPC Interaction: The system allowing players to converse with AI-controlled NPCs using natural language is functional, adding a significant layer of interactivity.
Image Generation Integration (Eden AI): Successfully integrating an image generation API to display location-specific images adds a strong visual element. Crucially, the successful implementation utilized the Eden AI API, which proved effective for generating the desired location visuals. The background generation approach keeps the UI responsive.
UI Enhancements: Iteratively developing the UI, including the hint system (click-to-reveal English/Romaji), the command buttons, action links, and the final "old school Japanese game" theme significantly improved usability and aesthetic appeal.
Debugging & Iteration: The process of identifying issues (like the look command regression, CSP errors, Git problems, formatting inconsistencies) and collaboratively implementing fixes was effective.
Problems I Still Have:
AI Output Consistency: While improved, the AI might still occasionally produce slightly suboptimal or unexpected results (e.g., descriptions that are too generic, vocabulary lists that don't perfectly match). Continuous monitoring and potential fallback mechanisms would be needed for a production system.
Vocabulary Alignment: The challenge of aligning AI-suggested vocabulary with the N5 list remains. The current system relies heavily on name/role matching, which isn't guaranteed to cover all relevant learning opportunities or AI suggestions.
Performance/Scalability: The application works well for a single user, but performance under load (multiple simultaneous players, frequent AI calls) hasn't been tested. AI API latency could become a bottleneck.
State Synchronization: While the image loading is handled in the background, more complex asynchronous updates might require more robust state synchronization between the backend and frontend.
Things I Want to Add:
TTS Integration for Audio Learning: A major enhancement would be adding text-to-speech (TTS) functionality. The idea is to allow users to click an icon next to Japanese text (item names, NPC names, vocabulary words in learn or theme commands) and hear the correct pronunciation.
Elaboration: This would cater to auditory learners and significantly improve the pronunciation aspect of language acquisition, making the game a more comprehensive learning tool. Technically, this involves choosing a TTS service (like OpenAI TTS, Google Cloud TTS, or potentially leveraging the service explored on the japanese-tts-service branch), creating a backend endpoint (likely in web_simple.py or a dedicated microservice) that accepts text and returns audio data (e.g., MP3). The frontend (game.js) would need speaker icons added dynamically next to relevant Japanese text elements. Clicking these icons would trigger an asynchronous request to the backend endpoint, receive the audio data, and play it using the browser's Audio object. This introduces challenges around managing audio playback, potential API costs, and ensuring responsiveness.
Spaced Repetition System (SRS): Integrate a proper SRS algorithm to manage vocabulary review, making learning more efficient than the simple (now removed) review command. This could involve tracking user performance on words.
Persistence: Implement user accounts and database storage to save player progress (location, inventory, learned vocabulary/SRS state) between sessions.
More Complex AI Scenarios: Introduce AI-driven puzzles, NPCs who remember past interactions more deeply, or AI that reacts to specific items the player is carrying.
Error Handling & Fallbacks: Improve robustness by adding more explicit error handling for failed AI API calls, providing clearer feedback to the user or using fallback content more gracefully.
